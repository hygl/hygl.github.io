---
title: "AI Modelle local laufen lassen und in VS Code nutzen"
categories: ["Editor", "VS Code","AI","KI","ollama"]
---
Um nicht vom Internetzugriff oder einem abhängig zu sein, kann man KI Modelle auch local laufen lassen und von dort aus in der IDE verwenden. Hier in diesem Beispiel verwende ich Ubuntu Linux, bei anderen Distributionen braucht man eventuell noch zusätzliche Pakete.

1. Um das KI Modell local zurverfügung zu stellen, verwendet man [ollama](https://ollama.ai/), das funktioniert ähnlich wie docker für ki modelle. `ollama run codellama:13b` führt zB ein KI Modell für Code local aus. Local Installiert werden kann ollama mit `curl https://ollama.ai/install.sh | sh`. Die Modelle laufen auch nur auf einer CPU, eine NVIDA GPU wirkt aber beschleunigend. Alternativ gibt es auch ein Docker Image, wenn man ollama nicht lokal installieren will.

1. [continue](https://continue.dev/) ist ein Plugin für JetBrain IDEs oder [Visual Studio Code](https://code.visualstudio.com/). Für VS Code kann man es über den Markplatz installieren. Lokal braucht man python, um den Server auszuführen.

1. In der Datei `~/.contiune/config.json` muss nun noch das passenden [Model](https://continue.dev/docs/reference/Model%20Providers/ollama) ergnänzt werden und konfiguriert werden, analog zum Beispiel oben also:

```json
{
    "models": [{
        "title": "Ollama",
        "provider": "ollama",
        "model": "llama2:13b",
    }]
}
```

In VS Code kann man nun mit `<Strg>+m` auf Continue zugreifen. Lokal ist es je nach Größe des Modells ohne NVIDIO GPU etwas langsam, es können aber auch Cloudmodelle wie ChatGPT verwendet werden. Der Code von codellama sieht auf den ersten Blick für einfache Übungen recht brauchbar aus.
